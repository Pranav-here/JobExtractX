import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig
from peft import PeftModel

# Load tokenizer (you uploaded it with the adapter)
tokenizer = AutoTokenizer.from_pretrained("alexlanxy/flan_t5_xl_lora_prompt_bf16_batch_2")

# 8-bit quantization config for QLoRA
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

# Load base model in 8-bit
base_model = AutoModelForSeq2SeqLM.from_pretrained(
    "google/flan-t5-xl",
    quantization_config=bnb_config,
    device_map="auto"  # Uses GPU if available
)

# Load your LoRA adapter
model = PeftModel.from_pretrained(base_model, "alexlanxy/flan_t5_xl_lora_prompt_bf16_batch_2")

# Put model in eval mode
model.eval()

# Example job posting
job_posting = """
  "description": "Outlier helps the worldâ€™s most innovative companies improve their AI models by providing human feedback. Are you an experienced software engineer who would like to lend your coding expertise to train AI models?We partner with organizations to train AI large language models, helping cutting-edge generative AI models write better code. Projects typically include discrete, highly variable problems that involve engaging with these models as they learn to code. There is no requirement for previous AI experience.About the opportunity:Outlier is looking for talented coders to help train generative artificial intelligence modelsThis freelance opportunity is remote and hours are flexible, so you can work whenever is best for youYou may contribute your expertise byâ€¦Crafting and answering questions related to computer science in order to help train AI modelsEvaluating and ranking code generated by AI modelsExamples of desirable expertise:Currently enrolled in or completed a bachelor's degree or higher in computer science at a selective institutionProficiency working with one or more of the the following languages: Java, Python, JavaScript / TypeScript, C++, Swift, and VerilogExcellent attention to detail, including grammar, punctuation, and style guidelinesPayment:Currently, pay rates for core project work by coding experts range from USD $16 to $33 per hour.Rates vary based on expertise, skills assessment, location, project need, and other factors. For example, higher rates may be offered to PhDs. For non-core work, such as during initial project onboarding or project overtime phases, lower rates may apply. Certain projects offer incentive payments. Please review the payment terms for each project.",
  "formattedLocation": "New Jersey, United States",
  "title": "Software Engineering Expertise for AI Training",
"""

# Build prompt with schema
schema = """
{
  "experience_level": "",
  "employment_status": [],
  "work_location": "",
  "salary": {"min": "", "max": "", "period": "", "currency": ""},
  "benefits": [],
  "job_functions": [],
  "required_skills": {
    "programming_languages": [],
    "tools": [],
    "frameworks": [],
    "databases": [],
    "other": []
  },
  "required_certifications": [],
  "required_minimum_degree": "",
  "required_experience": "",
  "industries": [],
  "additional_keywords": []
}
"""

prompt = (
    f"Label the following job posting in pure JSON format based on this example schema. "
    f"If no information for a field, leave the field blank.\n\n"
    f"Example schema:\n{schema}\n\n"
    f"Job posting:\n{job_posting}"
)

# Tokenize input
inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1536).to(model.device)

# Generate output
with torch.no_grad():
    outputs = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_length=384,
        num_beams=4,
        early_stopping=True
    )

# Decode and print
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("ðŸ“¦ Extracted JSON:\n", generated_text)
