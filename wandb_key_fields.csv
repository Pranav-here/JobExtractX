,Name,End Time,_name_or_path,model/num_parameters,peft_config.default.lora_alpha,peft_config.default.r,peft_config.default.lora_dropout,quantization_config.load_in_8bit,quantization_config.load_in_4bit,learning_rate,per_device_train_batch_size,gradient_accumulation_steps,num_train_epochs,gradient_checkpointing,optim,train/loss,train_loss,train/grad_norm,train/global_step,train_runtime,train_samples_per_second,train_steps_per_second,eval/loss,eval/samples_per_second,eval/steps_per_second
0,mistral-7b-lora-training-single-node,2025-04-16T00:35:45.000Z,mistralai/Mistral-7B-Instruct-v0.3,7251431424,16.0,8.0,0.1,True,False,0.0002,1,16,3,False,paged_adamw_8bit,0.9061,0.9523297822999288,0.4571378529071808,1866,201777.6623,0.148,0.009,1.07928466796875,0.528,0.528
1,flan-t5-xl-lora-training-rtx6000,2025-04-12T15:46:44.000Z,google/flan-t5-xl,2854475776,16.0,8.0,0.1,True,False,0.0002,4,4,3,False,paged_adamw_8bit,0.7232,0.875237967806997,0.8427208065986633,1866,77018.6534,0.388,0.024,0.1608970463275909,1.423,0.356
2,flan-t5-xl-lora-training-rtx6000,2025-04-12T15:41:02.000Z,google/flan-t5-xl,2854475776,16.0,8.0,0.1,True,False,0.0002,2,8,3,False,paged_adamw_8bit,1.4499,1.743098221152286,1.5342320203781128,1866,80098.8088,0.373,0.023,0.1607223451137542,1.357,0.679
3,flan-t5-job-training-rtx6000-prompt-ordered-fields,2025-04-11T11:52:24.000Z,google/flan-t5-large,783150080,,,,,,3e-05,1,16,3,False,adafactor,2.8019,3.39317800556604,5.00451135635376,1866,26408.0559,1.131,0.071,0.1628801077604293,2.951,2.951
